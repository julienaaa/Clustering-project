{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ply90PzqaHL"
      },
      "outputs": [],
      "source": [
        "!pip install -q kaggle\n",
        "\n",
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"xxxxxx\"\n",
        "os.environ['KAGGLE_KEY'] = \"xxxxxx\"\n",
        "\n",
        "!kaggle datasets download -d mohamedbakhet/amazon-books-reviews"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded_files = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "ryGpQJZ_qlu3",
        "outputId": "099ef5a5-2e9d-47d4-c11c-18fb67395e4e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-49f056f0-9a60-4972-86a9-5e73ddfe4bfc\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-49f056f0-9a60-4972-86a9-5e73ddfe4bfc\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Books_rating_part1.csv to Books_rating_part1.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def load_and_prepare_text_data(file_path,column_name='review/text',max_samples=10000):\n",
        "    \"\"\"I upload the csv file\n",
        "    I prepare a list of cleaned texts from a specific column\n",
        "    regarding the parameters:\n",
        "    file_path (str): this is the path to the CSV file containing the reviews\n",
        "    column_name (str): this is the name of the column containing the text (review/text)\n",
        "    max_samples (int): this is the maximum number of texts to keep (to limit memory)\n",
        "    at the end, I return a list of cleaned texts, that is to say a character string\"\"\"\n",
        "\n",
        "    #I load the csv file only with the column of interest\n",
        "    dataframe =pd.read_csv(file_path,usecols=[column_name])\n",
        "    #I delete rows with missing values ​​(NaN) in this column\n",
        "    dataframe =dataframe.dropna()\n",
        "    #I retrieve the first N texts as a list (up to max_samples)\n",
        "    cleaned_texts =dataframe[column_name].iloc[:max_samples].tolist()\n",
        "    return cleaned_texts\n",
        "\n",
        "def compute_tfidf_matrix(text_data,vocab_limit=1000):\n",
        "    \"\"\"I transform a list of texts into a TF-IDF matrix\n",
        "    for settings:\n",
        "    text_data (list of str): this is the list of text documents\n",
        "    vocab_limit (int): this is the maximum number of words retained in the vocabulary (by frequency)\n",
        "    in the end, I return\n",
        "    sparse matrix: TF-IDF matrix (n_documents x vocab_limit)\"\"\"\n",
        "\n",
        "    #I configure the TF-IDF vectorizer:\n",
        "    #max_features limits the number of words\n",
        "    #stop_words removes unnecessary words in English (\"the\", \"and\"...)\n",
        "    #lowercase forces lowercase\n",
        "    tfidf_model =TfidfVectorizer(max_features=vocab_limit,stop_words='english',lowercase=True)\n",
        "    #This is the training of the model and the transformation of texts into TF-IDF vectors\n",
        "    tfidf_result =tfidf_model.fit_transform(text_data)\n",
        "    return tfidf_result\n",
        "\n",
        "\n",
        "#I load and preprocess texts from the CSV file\n",
        "text_reviews =load_and_prepare_text_data('Books_rating_part1.csv',max_samples=10000)\n",
        "#This is the calculation of the TF-IDF matrix from the preprocessed texts\n",
        "tfidf_matrix =compute_tfidf_matrix(text_reviews)\n",
        "#I displays the shape (dimensions) of the generated TF-IDF matrix\n",
        "print(\"TF-IDF matrix shape:\",tfidf_matrix.shape)"
      ],
      "metadata": {
        "id": "9gEInKm1ql59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for svd dimension reduction\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def reduce_dimensionality_tfidf(tfidf_matrix,target_dims=2,random_state=42):\n",
        "    \"\"\"I want to reduce the dimension of the TF-IDF matrix with the Truncated SVD method\n",
        "    It is analogous to PCA for sparse data\n",
        "    for my settings:\n",
        "    tfidf_matrix (sparse matrix): this is the TF-IDF matrix to reduce\n",
        "    target_dims (int): this is the number of dimensions desired after reduction (default: 2)\n",
        "    random_state (int): this is the random seed to ensure reproducibility\n",
        "    at the end I return ndarray: reduced 2D matrix (or target_dims)\"\"\"\n",
        "\n",
        "    #I create the SVD transformer with the target number of dimensions\n",
        "    svd_transformer =TruncatedSVD(n_components=target_dims,random_state=random_state)\n",
        "    #I apply dimension reduction to the TF-IDF matrix\n",
        "    reduced_data =svd_transformer.fit_transform(tfidf_matrix)\n",
        "    return reduced_data\n",
        "\n",
        "def plot_2d_projection(data_2d,title=\"2D svd projection of TF-IDF reviews\"):\n",
        "    \"\"\"I display a 2D scatter plot of the projected data\n",
        "    for my settings:\n",
        "    data_2d (ndarray): this is the 2D matrix where each row is a projected document (SVD)\n",
        "    title (str): this is the title of the graph\"\"\"\n",
        "\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.scatter(data_2d[:,0],data_2d[:,1],s=10,alpha=0.5)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Component 1\")\n",
        "    plt.ylabel(\"Component 2\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "#I reduce the TF-IDF matrix to 2 dimensions for visualization\n",
        "projected_text_data =reduce_dimensionality_tfidf(tfidf_matrix)\n",
        "#I create the visualization of the 2D projection of text documents\n",
        "plot_2d_projection(projected_text_data)"
      ],
      "metadata": {
        "id": "mQlvjn76qmEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "#to calculate distances between points\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "def run_custom_kmeans(data,num_clusters=5,max_iterations=100,convergence_tol=1e-4,seed=42):\n",
        "    \"\"\"I implement the K-Means algorithm\n",
        "    for my settings:\n",
        "    data (ndarray): this is the matrix containing the data to be grouped\n",
        "    num_clusters (int): this is the number of clusters to form\n",
        "    max_iterations (int): this is the maximum number of iterations\n",
        "    convergence_tol (float): this is the convergence threshold based on the displacement of the centroids\n",
        "    seed (int): this is the random seed for reproducibility\n",
        "    in the end I return\n",
        "    cluster_assignments (ndarray): this is the cluster label assigned to each point\n",
        "    centroids (ndarray): these are the final coordinates of the centroids\"\"\"\n",
        "\n",
        "    #I initialize the random generator\n",
        "    np.random.seed(seed)\n",
        "    #I randomly choose points as initial centroids\n",
        "    initial_indices =np.random.choice(len(data),num_clusters,replace=False)\n",
        "    centroids =data[initial_indices]\n",
        "\n",
        "    for iteration in range(max_iterations):\n",
        "        #I assign each point to the nearest centroid\n",
        "        distance_matrix =euclidean_distances(data,centroids)\n",
        "        cluster_assignments =np.argmin(distance_matrix,axis=1)\n",
        "\n",
        "        #I recalculate the centroids as the average of the cluster points\n",
        "        updated_centroids =np.array([\n",
        "            data[cluster_assignments ==cluster_idx].mean(axis=0)\n",
        "            #I avoid empty clusters\n",
        "            if np.any(cluster_assignments ==cluster_idx)\n",
        "            else centroids[cluster_idx]\n",
        "            for cluster_idx in range(num_clusters)\n",
        "        ])\n",
        "        #I check the convergence (if the centroids no longer move)\n",
        "        centroid_shift =np.linalg.norm(updated_centroids-centroids,axis=1)\n",
        "        if np.all(centroid_shift <convergence_tol):\n",
        "            print(f\"converged at iteration {iteration}\")\n",
        "            break\n",
        "\n",
        "        centroids =updated_centroids\n",
        "    else:\n",
        "        print(\"reached maximum number of iterations without full convergence.\")\n",
        "\n",
        "    return cluster_assignments,centroids\n",
        "\n",
        "def visualize_clusters(data_2d,cluster_labels,cluster_centers=None,title=\"Cluster visualization (2D)\"):\n",
        "    \"\"\"I display the clusters on a 2D graph using seaborn and matplotlib\n",
        "    for my settings:\n",
        "    data_2d (ndarray): this is the projected data in 2D\n",
        "    cluster_labels (ndarray): These are the cluster labels for each point\n",
        "    cluster_centers (ndarray or None): these are the coordinates of the centroids\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10,6))\n",
        "    #This is the cloud of points colored according to the cluster\n",
        "    sns.scatterplot(x=data_2d[:,0],y=data_2d[:,1],hue=cluster_labels,palette='Set2',s=20,alpha=0.8,legend='full')\n",
        "\n",
        "    #I display the centroids if they are available\n",
        "    if cluster_centers is not None:\n",
        "        plt.scatter(cluster_centers[:,0],cluster_centers[:,1],c='black',s=150,marker='X',label='Centroids')\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Component 1\")\n",
        "    plt.ylabel(\"Component 2\")\n",
        "    plt.legend(title=\"Cluster\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "#I checked that the projected data is indeed in 2 dimensions so that it can be plotted.\n",
        "assert projected_text_data.shape[1] ==2,\"input data must be 2-dimensional for visualization\"\n",
        "#I run K-Means on the projected data\n",
        "num_k =5 #This is the number of clusters I want to have\n",
        "cluster_labels,cluster_centers =run_custom_kmeans(projected_text_data,num_clusters=num_k)\n",
        "#I display the result of the clusters\n",
        "visualize_clusters(projected_text_data,cluster_labels,cluster_centers)"
      ],
      "metadata": {
        "id": "ERxdGEucqmP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#It is to assess the quality of the clusters\n",
        "from sklearn.metrics import silhouette_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def evaluate_kmeans_performance(data,max_clusters=10):\n",
        "    \"\"\"I evaluate the performance of K-Means for different values ​​of k by calculating the inertia, i.e. the sum of intra-cluster distances\n",
        "    I also calculate the silhouette score, that is to say the quality of the separation between clusters\n",
        "    for my settings:\n",
        "    data (ndarray): this is the data set (2D) to be grouped\n",
        "    max_clusters (int): this is the maximum number of clusters to test\n",
        "    at the end I return a tuple\n",
        "    k_values ​​(list): this is the list of k values ​​tested\n",
        "    inertia_list (list): this is the list of corresponding inertias\n",
        "    silhouette_list (list): this is the list of matching silhouette scores\"\"\"\n",
        "\n",
        "    #my list for storing inertia\n",
        "    inertia_list =[]\n",
        "    #my list for storing silhouette scores\n",
        "    silhouette_list =[]\n",
        "    #the values ​​of k to test and I start at 2\n",
        "    k_values =list(range(2,max_clusters +1))\n",
        "\n",
        "    for k in k_values:\n",
        "        #I apply k-means with k clusters\n",
        "        cluster_labels,cluster_centers =run_custom_kmeans(data,num_clusters=k)\n",
        "        #I calculate the inertia, that is to say the sum of the distances between each point and its centroid\n",
        "        total_inertia =sum(\n",
        "            np.linalg.norm(data[cluster_labels == i] -cluster_centers[i],axis=1).sum()\n",
        "            for i in range(k)\n",
        "        )\n",
        "        inertia_list.append(total_inertia)\n",
        "\n",
        "        #I calculate the silhouette score, if I have at least 2 different clusters\n",
        "        if len(set(cluster_labels)) >1:\n",
        "            score =silhouette_score(data,cluster_labels)\n",
        "            silhouette_list.append(score)\n",
        "        else:\n",
        "            #the score is undefined if there is only one cluster\n",
        "            silhouette_list.append(None)\n",
        "\n",
        "    return k_values,inertia_list,silhouette_list\n",
        "\n",
        "def plot_kmeans_evaluation(k_values,inertias,silhouettes):\n",
        "    \"\"\"I plot k-means evaluation curves, such as the Elbow (inertia) curve and the silhouette score curve\n",
        "    for my settings:\n",
        "    k_values ​​(list): these are the values ​​of k used\n",
        "    inertias (list): these are the inertias measured for each k\n",
        "    silhouettes (list): these are the silhouette scores for each k\"\"\"\n",
        "\n",
        "    #I display the Elbow curve, that is to say the inertia as a function of k\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.plot(k_values,inertias,marker='o')\n",
        "    plt.title(\"Elbow method: inertia vs number of clusters\")\n",
        "    plt.xlabel(\"Number of clusters (k)\")\n",
        "    plt.ylabel(\"Inertia (within-cluster sum of squares)\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    #I display the silhouette score curve\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.plot(k_values,silhouettes,marker='s',color='green')\n",
        "    plt.title(\"Silhouette score vs number of clusters\")\n",
        "    plt.xlabel(\"Number of clusters (k)\")\n",
        "    plt.ylabel(\"Silhouette score\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "#I evaluate the performance of k-means on projected (2D) data\n",
        "k_range,inertia_scores,silhouette_scores =evaluate_kmeans_performance(projected_text_data,max_clusters=10)\n",
        "\n",
        "#I generate the evaluation graphs\n",
        "plot_kmeans_evaluation(k_range,inertia_scores,silhouette_scores)"
      ],
      "metadata": {
        "id": "fX9RPg450rG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "#for converting text to TF-IDF vectors\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "#for dimension reduction\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "#for clustering for the first pass\n",
        "from sklearn.cluster import KMeans\n",
        "#for distance calculations\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "#my configuration settings for BFR\n",
        "chunk_size =2000 #This is the number of documents processed per batch.\n",
        "reduced_dim =20 #This is the target dimension after reduction (SVD)\n",
        "distance_threshold =1.5 #This is the distance threshold for assigning a point to a cluster\n",
        "initial_cluster_count =5 #This is the number of clusters for the first batch\n",
        "\n",
        "#I create a dictionary to store statistics by cluster (DS: discard set)\n",
        "summary_clusters ={}\n",
        "\n",
        "def add_point_to_cluster(cluster_index,vector):\n",
        "    \"\"\"I update the statistics of a cluster with a new point\n",
        "    for my settings:\n",
        "    cluster_index (int): this is the cluster identifier\n",
        "    vector (ndarray): this is the vector representation of the document\"\"\"\n",
        "    if cluster_index not in summary_clusters:\n",
        "        #if the cluster does not yet exist, I initialize it\n",
        "        summary_clusters[cluster_index] ={\n",
        "            #the shadow of points\n",
        "            'N':1,\n",
        "            #the sum of the vectors\n",
        "            'SUM':vector.copy(),\n",
        "            #the sum of squares (for variance)\n",
        "            'SUMSQ':vector **2\n",
        "        }\n",
        "    else:\n",
        "        #I update existing statistics\n",
        "        summary_clusters[cluster_index]['N'] +=1\n",
        "        summary_clusters[cluster_index]['SUM'] +=vector\n",
        "        summary_clusters[cluster_index]['SUMSQ'] +=vector **2\n",
        "\n",
        "def compute_cluster_centroid_std(cluster_stats):\n",
        "    \"\"\"I calculate the centroid and standard deviation for a given cluster\n",
        "    for my settings:\n",
        "    cluster_stats (dict): this contains 'N', 'SUM' and 'SUMSQ'\n",
        "    at the end I return a tuple: centroid (vector), standard deviation (vector)\n",
        "    \"\"\"\n",
        "    N =cluster_stats['N']\n",
        "    SUM =cluster_stats['SUM']\n",
        "    SUMSQ =cluster_stats['SUMSQ']\n",
        "    centroid =SUM/N\n",
        "    std_dev =np.sqrt((SUMSQ/N)-(centroid**2))\n",
        "    return centroid,std_dev\n",
        "\n",
        "#my list to store unassigned points (RS: retained set)\n",
        "buffered_points =[]\n",
        "#I read in streaming of the csv, in blocks of lines\n",
        "text_chunk_reader =pd.read_csv(\"Books_rating_part1.csv\",usecols=[\"review/text\"],chunksize=chunk_size)\n",
        "#I initialize the TF-IDF vectorizer\n",
        "tfidf_builder =TfidfVectorizer(max_features=1000,stop_words='english',lowercase=True)\n",
        "#I train the vocabulary on a small initial sample (500 documents)\n",
        "initial_sample =pd.read_csv(\"Books_rating_part1.csv\",usecols=[\"review/text\"],nrows=500).dropna()\n",
        "tfidf_builder.fit(initial_sample[\"review/text\"].tolist())\n",
        "#I initialized the SVD reducer to go from TF-IDF to a reduced dimension\n",
        "svd_reducer =TruncatedSVD(n_components=reduced_dim,random_state=42)\n",
        "\n",
        "#I process each batch of documents\n",
        "for batch_index, chunk_df in enumerate(text_chunk_reader):\n",
        "    documents =chunk_df[\"review/text\"].dropna().tolist()\n",
        "    if not documents:\n",
        "        continue\n",
        "\n",
        "    #I do the TF-IDF conversion + dimension reduction\n",
        "    tfidf_matrix =tfidf_builder.transform(documents)\n",
        "    reduced_matrix =svd_reducer.fit_transform(tfidf_matrix)\n",
        "\n",
        "    #I initialize clustering on the first batch\n",
        "    if batch_index ==0:\n",
        "        #I use kmeans to initialize the first clusters\n",
        "        initial_kmeans =KMeans(n_clusters=initial_cluster_count,random_state=42).fit(reduced_matrix)\n",
        "        for cluster_id in range(initial_cluster_count):\n",
        "            cluster_vectors =reduced_matrix[initial_kmeans.labels_ ==cluster_id]\n",
        "            for vec in cluster_vectors:\n",
        "                #I add it to the cluster\n",
        "                add_point_to_cluster(cluster_id, vec)\n",
        "        print(\"initial cluster summaries created from first data batch\")\n",
        "        #I skip the normal assignment step for this batch\n",
        "        continue\n",
        "\n",
        "    #I am taring the following batches: assignment or buffering\n",
        "    for vec in reduced_matrix:\n",
        "        is_assigned =False\n",
        "\n",
        "        for cluster_id,stats in summary_clusters.items():\n",
        "            centroid,std_dev =compute_cluster_centroid_std(stats)\n",
        "            #This is the Mahalanobis type standard (simplified,diagonal only)\n",
        "            distance =np.linalg.norm((vec-centroid)/(std_dev+1e-6))\n",
        "\n",
        "            if distance<distance_threshold:\n",
        "                add_point_to_cluster(cluster_id,vec)\n",
        "                is_assigned =True\n",
        "                #If I find a nearby cluster, I go out\n",
        "                break\n",
        "\n",
        "        if not is_assigned:\n",
        "            #if it is too far from all clusters, I buffer (RS)\n",
        "            buffered_points.append(vec)\n",
        "\n",
        "    print(f\"batch {batch_index +1} processed —unassigned points (RS) {len(buffered_points)}\")\n",
        "\n",
        "#I summarize once all the batches have been processed\n",
        "print(f\"\\nfinal number of clusters (DS):{len(summary_clusters)}\")\n",
        "print(f\"remaining unassigned points (RS):{len(buffered_points)}\\n\")\n",
        "\n",
        "#This is a preview of the final cluster centroids (first 5 dimensions only)\n",
        "for cluster_id,stats in summary_clusters.items():\n",
        "    preview_centroid =(stats['SUM'] /stats['N'])[:5]\n",
        "    print(f\"cluster {cluster_id} —size:{stats['N']} —centroid preview:{preview_centroid}\")"
      ],
      "metadata": {
        "id": "ttbPDQvX0wgJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}